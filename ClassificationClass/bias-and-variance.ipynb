{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias / Variance Tradeoff and Test Error\n",
    " \n",
    "Consider the situation where we are trying to model a function $y = f(x)$ and we learn a model $\\hat{f}(x)$ based on the principle of minimizing MSE -- that is minimize \n",
    "$$E[(f(x) - \\hat{f}(x))^2]$$ over all $x$ (not just the training set).\n",
    "\n",
    "Mathematically MSE can be decomposed as follows: \n",
    "\n",
    "$$E[(f(x) - \\hat{f}(x))^2] = {\\rm Bias}(\\hat{f}(x))^2 + {\\rm Variance}[\\hat{f}] + \\sigma^2$$\n",
    "\n",
    "where $\\sigma^2$ is called *irreducible error*.   So when we are comparing various models -- candidate $\\hat{f}(x)$ -- we can (and must) accept the tradeoff between bias and variance.\n",
    "\n",
    "#### Bias\n",
    "\n",
    "Error caused by bias is calculated as the difference between the expected prediction of our model and the correct value we are trying to predict:\n",
    "\n",
    "$$Bias = E[\\hat{f}(x)] - E[f(x)]$$\n",
    "\n",
    "This quantity is the extent to which the mean of our predictions deviates from the mean of the observed values.   Remember this is measured over the \"whole population\" and not just the training set.  \n",
    "\n",
    "Bias is the error caused by our choice of model -- for example we used a linear model, but $f$ is not exactly linear.\n",
    "\n",
    "#### Variance\n",
    "\n",
    "Error caused by variance is taken as the variability of a model prediction for a given point:\n",
    "\n",
    "$$Variance = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "Notice that variance is defined in terms of the model $\\hat{f}$ and does not include the \"real\" function $f$.  Variance measures the extent to which $\\hat{f}$ moves around its mean.   Intuitively, using more training examples improves bias, but introduces more variance.\n",
    "\n",
    "\n",
    "As model complexity **increases**:\n",
    "- Bias **decreases**. (The model can more accurately model complex structure in data.  If the model predicts every point perfectly, bias is 0.)\n",
    "- Variance **increases**. (The model identifies more complex structures, making it more sensitive to small changes in the training data.)\n",
    " \n",
    "**Examples**\n",
    "\n",
    "- Predicting the mean value has high bias but 0 variance\n",
    "- Predicting on the training data exactly has 0 bias but high variance, since it assumes no regularity in the training data\n",
    "\n",
    "<br/> <br/>\n",
    "<img src=\"bias_v_variance.jpeg\" style=\"height: auto; width: 70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Implications for Training and Test Error\n",
    "\n",
    "- A more complex model can reduce the training error to 0, but then we expect the test error to rise, unless the test data looks *exactly like* the training data\n",
    "  * And if it did, then it wouldn't be an interesting learning problem.\n",
    "- On the other hand, a simple model may work relatively better on the test set because it is less \"committed\" to the specific values in the training set\n",
    "- We are looking for the minimizing point in this curve, but generally we can't get many test samples\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<img src=\"trainingVsTestError.png\" style=\"height: auto; width: 40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:120%\">Exercise</span>\n",
    "\n",
    "Where do each of these learning methods fall in terms of bias/variance tradeoff?\n",
    "\n",
    "1.  Predict to the mean:  learn the function $y = \\bar{y}$ for all $y$ in the training set \n",
    "2. 1 Nearest neighbor\n",
    "3. 20-Nearest neighbors\n",
    "4. Linear regression with one X variable\n",
    "5. Linear regression with all X variables\n",
    "5. Linear regression (biggest exponent 1)\n",
    "1. Polynomial regression (with exponent terms up to 8)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "### Exploring the Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"brain-and-body-weight-mammal-dataset\"></a>\n",
    "### Brain and Body Weight Mammal Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a [data set](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body (in kg) and the brain (in g) for 62 mammal species. We'll use this dataset to investigate bias vs. variance. Let's read it into Pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals = pd.read_table('mammals.txt', sep='\\t', names=['brain','body'], header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting way to get a scatter plot ... get a linear model plus scatter plot,\n",
    "#  and suppress the linear model :-)\n",
    "\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now suppose we discover a new species with body weight 100.  Use the model to predict its brain size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(mammals['body'].values.reshape(-1,1), mammals['brain'])\n",
    "print(lr.coef_)\n",
    "print(lr.predict(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how good is the prediction?  We don't really know because there is no point in our data set with the x value 100 -- and we have no measurement of how well the model does for data points outside the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reasoning About Training and Test Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's simulate a sampling scenario by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility -- a seeded random number generator always produces \n",
    "#  the same sequence of \"random numbers\" -- really helps with testing, but be sure to turn it off\n",
    "#  if you're doing simulation or etc.\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Randomly assign every observation to either universe 1 or universe 2.\n",
    "mammals['universe'] = np.random.randint(1, 3, len(mammals))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now tell Seaborn to create two plots in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The lines looks pretty similar between the two populations, despite the fact that they used completely separate samples of data. In both cases, we would predict a brain weight of about 45 g.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what was the point of this exercise? This was a visual demonstration of a high-bias, low-variance model.\n",
    "\n",
    "- It's **high bias** because it doesn't fit the data particularly well.\n",
    "- It's **low variance** because the model doesn't change much depending on which observations happen to be available in that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "### Let's Try Something Completely Different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What would a **low bias, high variance** model look like? Let's try polynomial regression with an eighth-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It's **low bias** because the models match the data effectively in both of the training sets\n",
    "- It's **high variance** because the models are widely different, depending on which observations happen to be available in that universe. (For a body weight of 100 kg, the brain weight prediction would be 40 kg in one universe and 0 kg in the other!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size=120%\">Exercise:</span>\n",
    "\n",
    "* Now relate this result back to training versus test MSE.   \n",
    "* Of the line or polynomial\n",
    "    * Which has the smaller training error\n",
    "    * Which has the smaller test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## Balancing Bias and Variance\n",
    "Can we find a middle ground?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perhaps we can create a model that has **less bias than the linear model** and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Let's try a second order polynomial instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=2);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This seems better. In both the left and right plots, **it fits the data well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance trade-off**: You are seeking a model that appropriately balances bias and variance and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want a model that best balances bias and variance. It\n",
    "should match our training data well (moderate bias) yet be low variance for out-of-sample data (moderate variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Training error as a function of\n",
    "complexity.\n",
    "- Question: Why do we even\n",
    "care about variance if we\n",
    "know we can generate a\n",
    "more accurate model with\n",
    "higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we obtain a zero-bias, zero-variance model?\n",
    "\n",
    "Only if \n",
    "1. You choose the right functional form for the data\n",
    "1. You have adequate training data\n",
    "1. The data has no irreducible error\n",
    "\n",
    "Remember MSE can be be broken down into\n",
    "1. Bias term\n",
    "1. Variance term\n",
    "1. Irreducible error\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
