{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN) Classification\n",
    "---\n",
    "\n",
    "K-nearest neighbors classification is (as its name implies) a classification model that uses the \"K\" most similar observations in order to make a prediction.\n",
    "\n",
    "KNN is a supervised learning method; therefore, the training data must have known target values.\n",
    "\n",
    "The process of of prediction using KNN is fairly straightforward:\n",
    "\n",
    "1. Pick a value for K.\n",
    "2. Search for the K observations in the data that are \"nearest\" to the measurements of the unknown iris.\n",
    "    - Euclidian distance is often used as the distance metric, but other metrics are allowed.\n",
    "3. Use the most popular response value from the K \"nearest neighbors\" as the predicted response value for the unknown iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations below show how a given area can change in its prediction as K changes.\n",
    "\n",
    "- This is simulated data with two predictors\n",
    "- Colored points represent true values and colored areas represent a **prediction space**. (This is called a Voronoi Diagram.)\n",
    "- Each prediction space is wgere the majority of the \"K\" nearest points are the color of the space.\n",
    "- To predict the class of a new point, we guess the class corresponding to the color of the space it lies in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=1)\n",
    "\n",
    "![1NN classification map](iris_01nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=5)\n",
    "\n",
    "![5NN classification map](iris_05nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=15)\n",
    "\n",
    "![15NN classification map](iris_15nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=50)\n",
    "\n",
    "![50NN classification map](iris_50nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as K increases, the classification spaces' borders become more distinct. However, you can also see that the spaces are not perfectly pure when it comes to the known elements within them.\n",
    "\n",
    "**How are outliers affected by K?** As K increases, outliers are \"smoothed out\". Look at the above three plots and notice how outliers strongly affect the prediction space when K=1. When K=50, outliers no longer affect region boundaries. This is a classic bias-variance tradeoff -- with increasing K, the bias increases but the variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:blue;font-size:125%\">\n",
    "- What happens when K $\\rightarrow$ number of points in the sample?\n",
    "</div>\n",
    "<div style=\"color:blue;font-size:125%\">\n",
    "- What is the best value for K?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NBA Position KNN Classifier\n",
    "\n",
    "This dataset containing the 2015 season statistics for ~500 NBA players. The columns we'll use for features (and the target 'pos') are:\n",
    "\n",
    "| Column | Meaning |\n",
    "| ---    | ---     |\n",
    "| pos | C: Center. F: Front. G: Guard |\n",
    "| ast | Assists per game | \n",
    "| stl | Steals per game | \n",
    "| blk | Blocks per game |\n",
    "| tov | Turnovers per game | \n",
    "| pf  | Personal fouls per game | \n",
    "\n",
    "**First look at the data file to see whether it fits that description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nba = pd.read_csv('NBA_players_2015.csv', usecols=['pos', 'ast', 'stl', 'blk', 'tov', 'pf'])\n",
    "print(nba.shape)\n",
    "print(nba.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the position categorical variables into numbers\n",
    "\n",
    "nba.pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that for a classifier we can have a Y that is not \n",
    "# numeric and do not need dummy variables.\n",
    "\n",
    "y = nba.pos\n",
    "X = nba.drop(columns=['pos'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a model\n",
    "For KNN, the choice of K is crucial, but we will ignore it for now, just choose k=3.\n",
    "First job is just to get a classifier running and evaluate it -- just like Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y, knn.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Setting n_neighbors to 1.  What do you expect to happen here?\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X, y)\n",
    "metrics.accuracy_score(y, knn.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Note:  Remember That the Classifier is Calculating a Conditional Probability\n",
    "\n",
    "Classifier will choose the class with highest probability, but knowing the underlying probability can be useful for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn15 = KNeighborsClassifier(n_neighbors=15)\n",
    "knn15.fit(X, y)\n",
    "knn15.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using the Train/Test Split Procedure\n",
    "\n",
    "* Remember we have been evaluating training error\n",
    "* To evaluate testing error, we can split training data into training set and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state = 333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn3 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And evaluate it on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, knn3.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now maybe 1 neighbor won't work as well?\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(X_train, y_train)\n",
    "metrics.accuracy_score(y_test, knn1.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparing Testing Accuracy With Null Accuracy (The Low Bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Null accuracy is the accuracy that can be achieved by **always predicting the most frequent class**. For example, if most players are Centers, we would always predict Center.\n",
    "\n",
    "The null accuracy is a benchmark against which you may want to measure every classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_class = y.value_counts().index[0]\n",
    "print(y.value_counts())\n",
    "print(most_freq_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()[most_freq_class] / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this we can get a 95% confidence interval on test accuracy\n",
    "(scores.mean() - 2 * scores.std(), scores.mean() + 2 * scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "scores = cross_val_score(knn, X, y, cv=10)\n",
    "(scores.mean() - 2*scores.std(), scores.mean() + 2 * scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a tradeoff in choosing number of cross-validation folds\n",
    "  * Fewer folds, faster, less aggressive use of test data (cv=1 is just a train/test split)\n",
    "  * More folds, slower.  cv=n-1 is \"all but one validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "for v in (2, 10, 20, 50, 100, 200):\n",
    "    cv = cross_val_score(knn, X, y, cv=v)\n",
    "    print (v, cv.mean(), cv.std())    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter optimization, or tuning,  means find an optimal value for K\n",
    "\n",
    "<span style=\"color:blue\">What does optimal mean?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 200, 4):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv = cross_val_score(knn, X, y, cv=10)\n",
    "    scores.append([k, cv.mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a plot of test accuracy as a function of k\n",
    "\n",
    "data = pd.DataFrame(scores,columns=['k','score'])\n",
    "data.plot.line(x='k',y='score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Since it looks like > 100 is less interesting, let's give it more focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 100, 1):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    cv = cross_val_score(knn, X, y, cv=10)\n",
    "    scores.append([k, cv.mean()])\n",
    "data = pd.DataFrame(scores,columns=['k','score'])\n",
    "data.plot.line(x='k',y='score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As K increases, why does the accuracy rise then fall?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Search for the \"best\" value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAINING ACCURACY and TESTING ACCURACY for K=1 through 358.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, random_state = 333)\n",
    "\n",
    "k_range = list(range(1, 100, 2))\n",
    "\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    y_pred_training = knn.predict(X_train)\n",
    "    training_accuracy = metrics.accuracy_score(y_train, y_pred_training)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    \n",
    "    # Calculate testing error.\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    testing_accuracy = metrics.accuracy_score(y_test, y_pred_test)\n",
    "    testing_accuracies.append(testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of K, training error, and testing error.\n",
    "column_dict = {'k': k_range, 'training_accuracy':training_accuracies, 'testing_accuracy':testing_accuracies}\n",
    "df = pd.DataFrame(column_dict).sort_values(by='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between k and training and test accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot( 'k', 'testing_accuracy', data=df, color='skyblue')\n",
    "plt.plot( 'k', 'training_accuracy', data=df, color='olive')\n",
    "plt.legend()\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-error-versus-testing-error\"></a>\n",
    "### Training Accuracy Versus Testing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that model complexity is greatest at $K=1$ -- as $K$ gets larger the model tends toward \"predict at the mode\"\n",
    "\n",
    "- **Training accuracy** increases as model complexity increases (lower value of K).\n",
    "- **Testing accuracy** will tend to be low (overfitting, too much complexity) then increase, then decrease (too little complexity)\n",
    "\n",
    "Evaluating the training and testing accuracy is important. For example:\n",
    "\n",
    "- If the training accuracy is much higher than the test accuracy, then our model is likely overfitting. \n",
    "- If the test accuracy starts decreasing as we vary a hyperparameter (K), we may be overfitting.\n",
    "- If either accuracy plateaus, our model is likely underfitting (not complex enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search:  \n",
    "\n",
    "We have this pattern now\n",
    "* Choose a hyperparameter\n",
    "* Fit a model using cross validation\n",
    "* Record accuracy for that model\n",
    "* Choose the hyperparameter that maximizes cross-val accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import grid_search\n",
    "parameters = {'n_neighbors': list(range(1,100))}\n",
    "knn = KNeighborsClassifier()\n",
    "clf = grid_search.GridSearchCV(knn, parameters)\n",
    "clf.fit(X, y)\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accuracies[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Classification\n",
    "* Train vs test error\n",
    "* Hyperparameter optimization\n",
    "  * Balancing *bias* versus *variance*\n",
    "* This procedure is exactly the same in scikit-learn regardless of the algorithm!\n",
    "\n",
    "**Advantages of KNN:**\n",
    "\n",
    "- It's simple to understand and explain.\n",
    "- Model training is fast.\n",
    "- It can be used for classification and regression (for regression, take the average value of the K nearest points!).\n",
    "- Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "**Disadvantages of KNN:**\n",
    "\n",
    "- It must store all of the training data.\n",
    "- Its prediction phase can be slow when n is large.\n",
    "- It is sensitive to irrelevant features.\n",
    "- It is sensitive to the scale of the data.\n",
    "- Accuracy is (generally) not competitive with the best supervised learning methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
